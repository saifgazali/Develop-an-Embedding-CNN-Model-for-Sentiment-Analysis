{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embeddings + CNN = Text Classification\n",
    "\n",
    "# Develop an embedding + CNN model for sentimental analysis \n",
    "\n",
    "import string,re\n",
    "from nltk.corpus import stopwords\n",
    "import os \n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    text = [t for t in tokens if t not in string.punctuation]\n",
    "    text = [t for t in text if t.isalpha()]\n",
    "    text = [t for t in text if t not in stopwords.words('english')]\n",
    "    text = [t.lower() for t in text]\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#laod doc and add to vocab\n",
    "def add_doc_to_vocab(filename,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    preprocessed_doc = clean_doc(doc)\n",
    "    vocab.update(preprocessed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory,vocab):\n",
    "    \n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        add_doc_to_vocab(path,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list to file\n",
    "def save_to_list(lines,filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "#add all docs to vocab \n",
    "process_docs('review_polarity/txt_sentoken/pos', vocab)\n",
    "process_docs('review_polarity/txt_sentoken/neg', vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36037\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23260\n"
     ]
    }
   ],
   "source": [
    "# keep tokens with minimum occurence\n",
    "min_occurence = 2\n",
    "\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurence]\n",
    "print(len(tokens))\n",
    "\n",
    "save_to_list(tokens,'vocab1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_filename = 'vocab1.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory, vocab, is_train):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "    # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc,vocab):\n",
    "    tokens = doc.split()\n",
    "    text = [t for t in tokens if t not in string.punctuation]\n",
    "    text = [t for t in text if t.isalpha()]\n",
    "    text = [t for t in text if t not in stopwords.words('english')]\n",
    "    text = [t.lower() for t in text]\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    text = [t for t in text if t in vocab]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    neg = process_docs('review_polarity/txt_sentoken/neg', vocab, is_train)\n",
    "    pos = process_docs('review_polarity/txt_sentoken/pos', vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = np.array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to encode each document as a sequence of integers. The Keras Embedding\n",
    "layer requires integer inputs where each integer maps to a single token that has a specific\n",
    "real-valued vector representation within the embedding. These vectors are random at the\n",
    "beginning of training, but during training become meaningful to the network. We can encode\n",
    "the training documents as sequences of integers using the Tokenizer class in the Keras API.\n",
    "First, we must construct an instance of the class then train it on all documents in the training\n",
    "dataset. In this case, it develops a vocabulary of all tokens in the training dataset and develops\n",
    "a consistent mapping from words in the vocabulary to unique integers. We could just as easily\n",
    "develop this mapping ourselves using our vocabulary file. The create tokenizer() function\n",
    "below will prepare a Tokenizer from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the mapping of words to integers has been prepared, we can use it to encode the\n",
    "reviews in the training dataset. We can do that by calling the texts to sequences() function\n",
    "on the Tokenizer. We also need to ensure that all documents have the same length. This is a\n",
    "requirement of Keras for efficient computation. We could truncate reviews to the smallest size\n",
    "or zero-pad (pad with the value 0) reviews to the maximum length, or some hybrid. In this case,\n",
    "we will pad all reviews to the length of the longest review in the training dataset. First, we can\n",
    "find the longest review using the max() function on the training dataset and take its length.\n",
    "We can then call the Keras function pad sequences() to pad the sequences to the maximum\n",
    "length by adding 0 values on the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def encode_seq(tokenzier,max_length,docs):\n",
    "    tokens = tokenizer.texts_to_sequences(docs)\n",
    "    \n",
    "    padded_seq = pad_sequences(tokens,maxlen=max_length,padding='post')\n",
    "    \n",
    "    return padded_seq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tokenizer' object has no attribute 'text_to_sequences'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-f60dd24a05e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_docs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mXtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mXtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-867315775334>\u001b[0m in \u001b[0;36mencode_seq\u001b[1;34m(tokenzier, max_length, docs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mencode_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenzier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpadded_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tokenizer' object has no attribute 'text_to_sequences'"
     ]
    }
   ],
   "source": [
    "train_docs , ytrain = load_clean_dataset(vocab,True)\n",
    "test_docs, ytest = load_clean_dataset(vocab,False)\n",
    "\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = encode_seq(tokenizer,max_length,train_docs)\n",
    "Xtest = encode_seq(tokenizer,max_length,test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13850\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define our neural network model. The model will use an Embedding\n",
    "layer as the first hidden layer. The Embedding layer requires the specification of the vocabulary\n",
    "size, the size of the real-valued vector space, and the maximum length of input documents. The\n",
    "vocabulary size is the total number of words in our vocabulary, plus one for unknown words.\n",
    "This could be the vocab set length or the size of the vocab within the tokenizer used to integer\n",
    "encode the documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,Conv1D,MaxPooling1D,Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a 100-dimensional vector space, but you could try other values, such as 50 or\n",
    "150. Finally, the maximum document length was calculated above in the max length variable\n",
    "used during padding. The complete model definition is listed below including the Embedding\n",
    "layer. We use a Convolutional Neural Network (CNN) as they have proven to be successful\n",
    "at document classification problems. A conservative CNN configuration is used with 32 filters\n",
    "(parallel fields for processing words) and a kernel size of 8 with a rectified linear (relu) activation\n",
    "function. This is followed by a pooling layer that reduces the output of the convolutional layer\n",
    "by half.\n",
    "Next, the 2D output from the CNN part of the model is flattened to one long 2D vector to\n",
    "represent the features extracted by the CNN. The back-end of the model is a standard Multilayer\n",
    "Perceptron layers to interpret the CNN features. The output layer uses a sigmoid activation\n",
    "function to output a value between 0 and 1 for the negative and positive sentiment in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size,max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,150,input_length = max_length))\n",
    "    model.add(Conv1D(filters=32,kernel_size=8,activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1186, 150)         2077500   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 1179, 32)          38432     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 589, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 18848)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                188490    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,304,433\n",
      "Trainable params: 2,304,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model(vocab_size,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 [==============================] - 22s 317ms/step - loss: 0.6922 - accuracy: 0.5520\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 19s 329ms/step - loss: 0.4745 - accuracy: 0.8296\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 20s 343ms/step - loss: 0.0690 - accuracy: 0.9826\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 26s 449ms/step - loss: 0.0098 - accuracy: 0.9997\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 17s 298ms/step - loss: 0.0034 - accuracy: 0.9995\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 18s 317ms/step - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 17s 293ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 16s 285ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 16s 286ms/step - loss: 8.1763e-04 - accuracy: 0.9996\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 17s 290ms/step - loss: 0.0013 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ebba5ca8e0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain,ytrain,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('embedding_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 4s 64ms/step - loss: 8.9404e-04 - accuracy: 1.0000\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.4101 - accuracy: 0.8850\n",
      "1.0\n",
      "0.8849999904632568\n"
     ]
    }
   ],
   "source": [
    "# Evaluating model\n",
    "\n",
    "lossTrain,accTrain = model.evaluate(Xtrain,ytrain)\n",
    "lossTest,accTest = model.evaluate(Xtest,ytest)\n",
    "\n",
    "print(accTrain)\n",
    "print(accTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify a review as negative or positive \n",
    "\n",
    "def predict_sentiment(review,vocab,tokenizer,max_length,model):\n",
    "    \n",
    "    preprocessed_doc = clean_doc(review,vocab)\n",
    "    \n",
    "    padded = encode_seq(tokenizer,max_length,[review])\n",
    "    \n",
    "    yhat = model.predict(padded)\n",
    "    \n",
    "    #retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "        return percent_pos, 'POSITIVE'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-f61af740da52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Everyone will enjoy this film. I love it, recommended!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpercent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Review: [%s]\\nSentiment: %s (%.3f%%)'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentiment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpercent\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [This is a bad movie. Do not watch it. It sucks.]\n",
      "Sentiment: NEGATIVE (55.143%)\n"
     ]
    }
   ],
   "source": [
    "# test negative text\n",
    "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram CNN model for sentimental analysis\n",
    "\n",
    "A standard deep learning model for text classification and sentiment analysis uses a word\n",
    "embedding layer and one-dimensional convolutional neural network. The model can be expanded\n",
    "by using multiple parallel convolutional neural networks that read the source document using\n",
    "different kernel sizes. This, in effect, creates a multichannel convolutional neural network for\n",
    "text that reads text with different n-gram sizes (groups of words). In this tutorial, you will\n",
    "discover how to develop a multichannel convolutional neural network for sentiment prediction\n",
    "on text movie review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "# save a dataset to file\n",
    "def save_dataset(dataset, filename):\n",
    "    dump(dataset, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train.pkl\n",
      "Saved: test.pkl\n"
     ]
    }
   ],
   "source": [
    "# save training datasets\n",
    "save_dataset([train_docs, ytrain], 'train.pkl')\n",
    "save_dataset([test_docs, ytest], 'test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop multichannel model\n",
    "# 1) Encode data\n",
    "\n",
    "from pickle import load \n",
    "def load_dataset(filename):\n",
    "    return load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLines,trainLabels = load_dataset('train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot two teen couples go church party drink drive get accident one guys dies girlfriend continues see life nightmares deal watch movie sorta find critique movie teen generation touches cool idea presents bad package makes review even harder one write since generally applaud films attempt break mold mess head lost highway memento good bad ways making types films folks one correctly seem taken pretty neat concept executed terribly problems movie well main problem simply jumbled starts normal fantasy world audience member idea going dreams characters coming back dead others look like dead strange apparitions chase scenes tons weird things happen simply explained personally mind trying unravel film every give clue get kind fed biggest problem obviously got big secret hide seems want hide completely final five minutes make things entertaining thrilling even engaging meantime really sad part arrow dig flicks like actually figured point strangeness start make little bit sense still make film entertaining guess bottom line movies like always make sure audience even given secret enter world understanding mean showing melissa sagemiller running away visions minutes throughout movie plain lazy okay get people chasing know really need see giving us different scenes offering insight strangeness going movie apparently studio took film away director shows pretty decent teen movie somewhere guess suits decided turning music video little edge would make sense actors pretty good part although wes bentley seemed playing exact character american beauty new neighborhood biggest kudos go sagemiller holds throughout entire film actually feeling overall film stick entertain confusing rarely feels pretty redundant runtime despite pretty cool ending explanation came oh way horror teen slasher flick look way someone apparently assuming genre still hot kids also wrapped production two years ago sitting shelves ever since whatever skip joblo coming nightmare elm street blair witch crow crow salvation lost highway memento others stir echoes\n"
     ]
    }
   ],
   "source": [
    "print(trainLines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, a multiple-input model can be defined using the functional API. We will define a\n",
    "model with three input channels for processing 4-grams, 6-grams, and 8-grams of movie review\n",
    "text. Each channel is comprised of the following elements:\n",
    " Input layer that defines the length of input sequences.\n",
    " Embedding layer set to the size of the vocabulary and 100-dimensional real-valued repre\u0002sentations.\n",
    " Conv1D layer with 32 filters and a kernel size set to the number of words to read at once.\n",
    " MaxPooling1D layer to consolidate the output from the convolutional layer.\n",
    " Flatten layer to reduce the three-dimensional output to two dimensional for concatenation.\n",
    "The output from the three channels are concatenated into a single vector and process by a\n",
    "Dense layer and an output layer. The function below defines and returns the model. As part of\n",
    "defining the model, a summary of the defined model is printed and a plot of the model graph is\n",
    "created and saved to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size,length):\n",
    "    \n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size,100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32,kernel_size=4,activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.4)(conv1)\n",
    "    maxpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(maxpool1)\n",
    "    \n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size,100)(inputs2)\n",
    "    conv2 = Conv1D(filters=32,kernel_size=6,activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    maxpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(maxpool2)\n",
    "    \n",
    "    #channel 3 \n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size,100)(inputs3)\n",
    "    conv3 = Conv1D(filters=32,kernel_size=8,activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    maxpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(maxpool3)\n",
    "    \n",
    "    #merge \n",
    "    merged = concatenate([flat1,flat2,flat3])\n",
    "    \n",
    "    #Dense layers\n",
    "    Dense1 = Dense(10,activation='relu')(merged)\n",
    "    outputs = Dense(1,activation='sigmoid')(Dense1)\n",
    "    \n",
    "    model = Model(inputs = [inputs1,inputs2,inputs3],outputs=outputs)\n",
    "    \n",
    "    #compile\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics = ['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the training data\n",
    "trainLines , trainLabels = load_dataset('train.pkl')\n",
    "testLines, testLabels = load_dataset('test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "may critic alive harbors much affection monster movies delighted entertainment ron yarn tremors even last yarn anaconda something films causes lower return saturday youth spent company creature black blob deep rising yarn quite pass test sure enough modern monster movie ingredients place conspicuously collection bait excuse characters isolated location derelict cruise ship south china sea comic relief least one big explosion elements like sleazy anthony heald also appears marine slinky international jewel thief famke janssen whose white cotton tank top hides heart gold happens deep rising noteworthy primarily mechanical manner spits ingredients terrorist crew led mercenary wes studi boat captain finnegan treat williams shows loot cruise ship sea monsters show eat mercenary crew survivors make closing credits go lights hard work much enthusiasm sort especially monster make laugh every time makes scream laughs provided almost entirely kevin generally amusing mechanic stephen sommers seems concerned creating tone menace something skeletons something gunfire special effects taken bit seriously deep rising missing one unmistakable cue expected ridiculous good time hide eyes case point comparing deep rising recent cousin anaconda deep rising one victims back view partially still alive horror freakish appearance pain moment bit disturbing laughable anaconda also see victim partially still alive looks another character make mistake deep rising anaconda beat heck comes technical pacing also gloomy uninspired nearly enough fun ask much monster movies ask act like monster movies show fantastically impressive massive beast tentacles show massive beast figure get point\n"
     ]
    }
   ],
   "source": [
    "print(testLines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(trainLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(t) for t in trainLines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = encode_seq(tokenizer,max_length,trainLines)\n",
    "Xtest = encode_seq(tokenizer,max_length,testLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 8711)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           [(None, 8711)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           [(None, 8711)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 8711, 100)    1385000     input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 8711, 100)    1385000     input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 8711, 100)    1385000     input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 8708, 32)     12832       embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 8706, 32)     19232       embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 8704, 32)     25632       embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 8708, 32)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 8706, 32)     0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 8704, 32)     0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 4354, 32)     0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 4353, 32)     0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 4352, 32)     0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 139328)       0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 139296)       0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 139264)       0           max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 417888)       0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           4178890     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            11          dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,391,597\n",
      "Trainable params: 8,391,597\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "import graphviz\n",
    "import pydot\n",
    "plot_model(model,show_shapes=True,to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 213s 2s/step - loss: 0.6950 - accuracy: 0.4944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2eb805fd400>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xtrain,Xtrain,Xtrain],trainLabels,epochs=1,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('N-gram-CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 5s 672ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate([Xtest,Xtest,Xtest],testLabels)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions \n",
    "\n",
    " Different n-grams. Explore the model by changing the kernel size (number of n-grams)\n",
    "used by the channels in the model to see how it impacts model skill.\n",
    "\n",
    " More or Fewer Channels. Explore using more or fewer channels in the model and see\n",
    "how it impacts model skill.\n",
    "\n",
    " Shared Embedding. Explore configurations where each channel shares the same word\n",
    "embedding and report on the impact on model skill.16.7. Further Reading 187\n",
    "\n",
    " Deeper Network. Convolutional neural networks perform better in computer vision\n",
    "when they are deeper. Explore using deeper models here and see how it impacts model\n",
    "skill.\n",
    "\n",
    " Truncated Sequences. Padding all sequences to the length of the longest sequence\n",
    "might be extreme if the longest sequence is very different to all other reviews. Study the\n",
    "distribution of review lengths and truncate reviews to a mean length.\n",
    "\n",
    " Truncated Vocabulary. We removed infrequently occurring words, but still had a large\n",
    "vocabulary of more than 25,000 words. Explore further reducing the size of the vocabulary\n",
    "and the effect on model skill.\n",
    "\n",
    " Epochs and Batch Size. The model appears to fit the training dataset quickly. Explore\n",
    "alternate configurations of the number of training epochs and batch size and use the test\n",
    "dataset as a validation set to pick a better stopping point for training the model.\n",
    "\n",
    " Pre-Train an Embedding. Explore pre-training a Word2Vec word embedding in the\n",
    "model and the impact on model skill with and without further fine tuning during training.\n",
    "\n",
    " Use GloVe Embedding. Explore loading the pre-trained GloVe embedding and the\n",
    "impact on model skill with and without further fine tuning during training.\n",
    "\n",
    " Train Final Model. Train a final model on all available data and use it make predictions\n",
    "on real ad hoc movie reviews from the internet.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
